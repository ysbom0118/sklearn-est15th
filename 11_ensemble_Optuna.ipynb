{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블 모델 - Optuna를 활용한 하이퍼파라미터 최적화\n",
    "\n",
    "## 학습 목표\n",
    "1. **Optuna**를 사용하여 앙상블 모델의 하이퍼파라미터를 자동으로 최적화할 수 있다\n",
    "2. **RandomForest, GradientBoosting, XGBoost, LightGBM** 모델을 Optuna로 튜닝할 수 있다\n",
    "3. **Pruning**을 활용하여 비효율적인 trial을 조기 종료할 수 있다\n",
    "4. 최적화된 앙상블 모델들의 성능을 비교 분석할 수 있다\n",
    "\n",
    "## 주요 내용\n",
    "- California Housing 데이터셋 활용 (회귀)\n",
    "- RandomForest, GradientBoosting, XGBoost, LightGBM 최적화\n",
    "- Optuna Pruning 기능 활용\n",
    "- 최적화 결과 비교 및 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치 (필요시)\n",
    "# !pip install optuna xgboost lightgbm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optuna\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 앙상블 모델\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "\n",
    "# XGBoost, LightGBM\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    print(\"XGBoost loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"XGBoost not installed. Run: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    print(\"LightGBM loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"LightGBM not installed. Run: pip install lightgbm\")\n",
    "\n",
    "print(f\"\\nOptuna version: {optuna.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# California Housing 데이터셋 로드\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y = housing.target\n",
    "\n",
    "print(f\"데이터 shape: {X.shape}\")\n",
    "print(f\"타겟 shape: {y.shape}\")\n",
    "print(f\"\\nFeature names: {housing.feature_names}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 스케일링 (트리 기반 모델은 필수는 아니지만 일관성을 위해)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"학습 데이터: {X_train_scaled.shape}\")\n",
    "print(f\"테스트 데이터: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장용 DataFrame\n",
    "results_df = pd.DataFrame(columns=['Model', 'MSE', 'RMSE', 'MAE', 'R2', 'Best_Params'])\n",
    "\n",
    "def evaluate_model(model_name, y_true, y_pred, best_params=None):\n",
    "    \"\"\"모델 평가 및 결과 저장\"\"\"\n",
    "    global results_df\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    new_row = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'MSE': [mse],\n",
    "        'RMSE': [rmse],\n",
    "        'MAE': [mae],\n",
    "        'R2': [r2],\n",
    "        'Best_Params': [str(best_params) if best_params else '-']\n",
    "    })\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R2: {r2:.4f}\")\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RandomForest + Optuna 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf(trial):\n",
    "    \"\"\"RandomForest의 objective 함수\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = RandomForestRegressor(**params)\n",
    "    \n",
    "    # 교차 검증\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train,\n",
    "                            cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    return -scores.mean()\n",
    "\n",
    "# Optuna Study 생성 및 실행\n",
    "print(\"=\" * 50)\n",
    "print(\"RandomForest 최적화 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "study_rf = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study_rf.optimize(objective_rf, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n최적 파라미터: {study_rf.best_params}\")\n",
    "print(f\"최적 MSE (CV): {study_rf.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 RandomForest 모델 학습 및 평가\n",
    "best_rf = RandomForestRegressor(**study_rf.best_params, random_state=42, n_jobs=-1)\n",
    "best_rf.fit(X_train_scaled, y_train)\n",
    "pred_rf = best_rf.predict(X_test_scaled)\n",
    "\n",
    "evaluate_model('RandomForest (Optuna)', y_test, pred_rf, study_rf.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GradientBoosting + Optuna 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_gb(trial):\n",
    "    \"\"\"GradientBoosting의 objective 함수\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = GradientBoostingRegressor(**params)\n",
    "    \n",
    "    scores = cross_val_score(model, X_train_scaled, y_train,\n",
    "                            cv=3, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    return -scores.mean()\n",
    "\n",
    "# Optuna Study 생성 및 실행\n",
    "print(\"=\" * 50)\n",
    "print(\"GradientBoosting 최적화 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "study_gb = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study_gb.optimize(objective_gb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n최적 파라미터: {study_gb.best_params}\")\n",
    "print(f\"최적 MSE (CV): {study_gb.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 GradientBoosting 모델 학습 및 평가\n",
    "best_gb = GradientBoostingRegressor(**study_gb.best_params, random_state=42)\n",
    "best_gb.fit(X_train_scaled, y_train)\n",
    "pred_gb = best_gb.predict(X_test_scaled)\n",
    "\n",
    "evaluate_model('GradientBoosting (Optuna)', y_test, pred_gb, study_gb.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBoost + Optuna 최적화 (with Pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb(trial):\n",
    "    \"\"\"XGBoost의 objective 함수 (Pruning 적용)\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 10, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10, log=True),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**params)\n",
    "    \n",
    "    scores = cross_val_score(model, X_train_scaled, y_train,\n",
    "                            cv=3, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    return -scores.mean()\n",
    "\n",
    "# Optuna Study 생성 및 실행 (Pruning 적용)\n",
    "print(\"=\" * 50)\n",
    "print(\"XGBoost 최적화 시작 (with Pruning)...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "\n",
    "study_xgb = optuna.create_study(direction='minimize', sampler=sampler, pruner=pruner)\n",
    "study_xgb.optimize(objective_xgb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n최적 파라미터: {study_xgb.best_params}\")\n",
    "print(f\"최적 MSE (CV): {study_xgb.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 XGBoost 모델 학습 및 평가\n",
    "best_xgb = XGBRegressor(**study_xgb.best_params, random_state=42, n_jobs=-1, verbosity=0)\n",
    "best_xgb.fit(X_train_scaled, y_train)\n",
    "pred_xgb = best_xgb.predict(X_test_scaled)\n",
    "\n",
    "evaluate_model('XGBoost (Optuna)', y_test, pred_xgb, study_xgb.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LightGBM + Optuna 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgbm(trial):\n",
    "    \"\"\"LightGBM의 objective 함수\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 10, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10, log=True),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': -1\n",
    "    }\n",
    "    \n",
    "    model = LGBMRegressor(**params)\n",
    "    \n",
    "    scores = cross_val_score(model, X_train_scaled, y_train,\n",
    "                            cv=3, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    return -scores.mean()\n",
    "\n",
    "# Optuna Study 생성 및 실행\n",
    "print(\"=\" * 50)\n",
    "print(\"LightGBM 최적화 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "study_lgbm = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n최적 파라미터: {study_lgbm.best_params}\")\n",
    "print(f\"최적 MSE (CV): {study_lgbm.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 LightGBM 모델 학습 및 평가\n",
    "best_lgbm = LGBMRegressor(**study_lgbm.best_params, random_state=42, n_jobs=-1, verbosity=-1)\n",
    "best_lgbm.fit(X_train_scaled, y_train)\n",
    "pred_lgbm = best_lgbm.predict(X_test_scaled)\n",
    "\n",
    "evaluate_model('LightGBM (Optuna)', y_test, pred_lgbm, study_lgbm.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 결과 출력\n",
    "print(\"=\" * 70)\n",
    "print(\"앙상블 모델 성능 비교 결과\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_summary = results_df[['Model', 'MSE', 'RMSE', 'MAE', 'R2']].sort_values('MSE').reset_index(drop=True)\n",
    "display(results_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 비교 시각화\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "models = results_summary['Model'].tolist()\n",
    "\n",
    "# MSE 비교\n",
    "ax1 = axes[0, 0]\n",
    "bars = ax1.barh(models, results_summary['MSE'], color=colors)\n",
    "ax1.set_xlabel('MSE (낮을수록 좋음)')\n",
    "ax1.set_title('모델별 MSE 비교')\n",
    "ax1.invert_yaxis()\n",
    "for i, v in enumerate(results_summary['MSE']):\n",
    "    ax1.text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=9)\n",
    "\n",
    "# RMSE 비교\n",
    "ax2 = axes[0, 1]\n",
    "bars = ax2.barh(models, results_summary['RMSE'], color=colors)\n",
    "ax2.set_xlabel('RMSE (낮을수록 좋음)')\n",
    "ax2.set_title('모델별 RMSE 비교')\n",
    "ax2.invert_yaxis()\n",
    "for i, v in enumerate(results_summary['RMSE']):\n",
    "    ax2.text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=9)\n",
    "\n",
    "# MAE 비교\n",
    "ax3 = axes[1, 0]\n",
    "bars = ax3.barh(models, results_summary['MAE'], color=colors)\n",
    "ax3.set_xlabel('MAE (낮을수록 좋음)')\n",
    "ax3.set_title('모델별 MAE 비교')\n",
    "ax3.invert_yaxis()\n",
    "for i, v in enumerate(results_summary['MAE']):\n",
    "    ax3.text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=9)\n",
    "\n",
    "# R2 비교\n",
    "ax4 = axes[1, 1]\n",
    "bars = ax4.barh(models, results_summary['R2'], color=colors)\n",
    "ax4.set_xlabel('R2 Score (높을수록 좋음)')\n",
    "ax4.set_title('모델별 R2 Score 비교')\n",
    "ax4.invert_yaxis()\n",
    "for i, v in enumerate(results_summary['R2']):\n",
    "    ax4.text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optuna 최적화 과정 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 모델의 최적화 히스토리 비교\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "studies = [\n",
    "    (study_rf, 'RandomForest'),\n",
    "    (study_gb, 'GradientBoosting'),\n",
    "    (study_xgb, 'XGBoost'),\n",
    "    (study_lgbm, 'LightGBM')\n",
    "]\n",
    "\n",
    "for ax, (study, name) in zip(axes.flatten(), studies):\n",
    "    trials = [t.value for t in study.trials if t.value is not None]\n",
    "    best_values = [min(trials[:i+1]) for i in range(len(trials))]\n",
    "    \n",
    "    ax.plot(trials, 'o-', alpha=0.5, label='Trial Value', markersize=3)\n",
    "    ax.plot(best_values, 'r-', linewidth=2, label='Best Value')\n",
    "    ax.set_xlabel('Trial')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title(f'{name} 최적화 히스토리')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 파라미터 중요도\n",
    "fig = optuna.visualization.plot_param_importances(study_xgb)\n",
    "fig.update_layout(title='XGBoost 파라미터 중요도')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM 파라미터 중요도\n",
    "fig = optuna.visualization.plot_param_importances(study_lgbm)\n",
    "fig.update_layout(title='LightGBM 파라미터 중요도')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 모델의 Feature Importance 비교\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "models_for_importance = [\n",
    "    (best_rf, 'RandomForest'),\n",
    "    (best_gb, 'GradientBoosting'),\n",
    "    (best_xgb, 'XGBoost'),\n",
    "    (best_lgbm, 'LightGBM')\n",
    "]\n",
    "\n",
    "feature_names = housing.feature_names\n",
    "\n",
    "for ax, (model, name) in zip(axes.flatten(), models_for_importance):\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    ax.barh(range(len(indices)), importances[indices], color='steelblue')\n",
    "    ax.set_yticks(range(len(indices)))\n",
    "    ax.set_yticklabels([feature_names[i] for i in indices])\n",
    "    ax.set_xlabel('Feature Importance')\n",
    "    ax.set_title(f'{name} Feature Importance')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 최적 파라미터 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"최적화된 하이퍼파라미터 요약\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_studies = [\n",
    "    ('RandomForest', study_rf),\n",
    "    ('GradientBoosting', study_gb),\n",
    "    ('XGBoost', study_xgb),\n",
    "    ('LightGBM', study_lgbm)\n",
    "]\n",
    "\n",
    "for name, study in all_studies:\n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(f\"  최적 MSE (CV): {study.best_value:.4f}\")\n",
    "    for param, value in study.best_params.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {param}: {value:.6f}\")\n",
    "        else:\n",
    "            print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델 선택\n",
    "best_model_row = results_summary.iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"최고 성능 모델\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"모델: {best_model_row['Model']}\")\n",
    "print(f\"MSE: {best_model_row['MSE']:.4f}\")\n",
    "print(f\"RMSE: {best_model_row['RMSE']:.4f}\")\n",
    "print(f\"MAE: {best_model_row['MAE']:.4f}\")\n",
    "print(f\"R2 Score: {best_model_row['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 결론\n",
    "\n",
    "### Optuna를 사용한 앙상블 모델 최적화 요약\n",
    "\n",
    "| 모델 | 주요 튜닝 파라미터 |\n",
    "|------|--------------------|\n",
    "| **RandomForest** | n_estimators, max_depth, min_samples_split, max_features |\n",
    "| **GradientBoosting** | n_estimators, learning_rate, max_depth, subsample |\n",
    "| **XGBoost** | n_estimators, learning_rate, max_depth, reg_alpha, reg_lambda |\n",
    "| **LightGBM** | n_estimators, learning_rate, num_leaves, min_child_samples |\n",
    "\n",
    "### Optuna 활용 팁\n",
    "\n",
    "1. **Pruning 활용**: XGBoost, LightGBM처럼 학습 시간이 긴 모델에 특히 효과적\n",
    "2. **로그 스케일 탐색**: learning_rate, reg_alpha 등은 `log=True`로 탐색\n",
    "3. **적절한 trial 수**: 파라미터가 많을수록 더 많은 trial 필요 (50~200)\n",
    "4. **시각화 활용**: 파라미터 중요도를 확인하여 불필요한 파라미터 제거 가능\n",
    "\n",
    "### 참고 자료\n",
    "- [Optuna 공식 문서](https://optuna.readthedocs.io/)\n",
    "- [XGBoost 파라미터 튜닝 가이드](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
    "- [LightGBM 파라미터 튜닝 가이드](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
